{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with variable length sequences in [Pytorch 1.1](https://pytorch.org/) with [Ignite](https://pytorch.org/ignite/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Process Data through pytorch Dataset  \n",
    "  * Tokenize tweets  \n",
    "  * Build vocabulary  \n",
    "  * Vectorize tweets  \n",
    "* Make batches through pytorch Dataloader  \n",
    "  * Pad tweets to the max length in the batch\n",
    "* Max Pooling and Average Pooling\n",
    "  * RNN model (GRU) with concat pooling\n",
    "* [Ignite](https://pytorch.org/ignite/) training callbacks\n",
    "  * Define ignite training loops\n",
    "  * Add callback for epoch loss and accuracy\n",
    "  * Add callback for ModelCheckpoint\n",
    "  * Add callback for EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T02:58:55.718366Z",
     "start_time": "2019-05-19T02:58:54.563456Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import re\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from tqdm.auto import tqdm, trange\n",
    "tqdm.pandas(desc='Progress')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import ignite\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T02:58:55.730335Z",
     "start_time": "2019-05-19T02:58:55.719364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "Pandas version: 0.22.0\n",
      "Pytorch version: 1.1.0\n",
      "Spacy version: 2.0.11\n",
      "Ignite version: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "print('Python version:',sys.version)\n",
    "print('Pandas version:',pd.__version__)\n",
    "print('Pytorch version:', torch.__version__)\n",
    "print('Spacy version:', spacy.__version__)\n",
    "print('Ignite version:', ignite.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Process Data through pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T02:58:55.853007Z",
     "start_time": "2019-05-19T02:58:55.731333Z"
    }
   },
   "outputs": [],
   "source": [
    "data_root = pathlib.Path('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T02:59:02.423437Z",
     "start_time": "2019-05-19T02:58:55.855002Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1262889, 4), (315723, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv in pandas dataframe\n",
    "df = pd.read_csv(data_root / 'Sentiment Analysis Dataset.csv', error_bad_lines=False)\n",
    "\n",
    "# split the data into train and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[['Sentiment']])\n",
    "train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom class for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T02:59:02.435404Z",
     "start_time": "2019-05-19T02:59:02.425433Z"
    }
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Define the pytorch Dataset to process the tweets\n",
    "       This class can be used for both training and validation dataset\n",
    "       Run it for training data and pass the word2idx and idx2word when running\n",
    "       for validation data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, word2idx=None, idx2word=None, max_vocab_size=50000):\n",
    "        print('Processing Data')\n",
    "        self.df = df\n",
    "        print('Removing white space...')\n",
    "        self.df.SentimentText = self.df.SentimentText.progress_apply(lambda x: x.strip())\n",
    "        self.nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "        if word2idx is None:\n",
    "            print('Building Counter...')\n",
    "            word_counter = self.build_counter()\n",
    "            print('Building Vocab...')\n",
    "            self.word2idx, self.idx2word = self.build_vocab(word_counter, max_vocab_size)\n",
    "        else:\n",
    "            self.word2idx, self.idx2word = word2idx, idx2word\n",
    "        print('*'*100)\n",
    "        print('Dataset info:')\n",
    "        print(f'Number of Tweets: {self.df.shape[0]}')\n",
    "        print(f'Vocab Size: {len(self.word2idx)}')\n",
    "        print('*'*100)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.df.SentimentText[idx]\n",
    "        tokens = [w.text.lower() for w in self.nlp(self.tweet_clean(sent))]\n",
    "        vec = self.vectorize(tokens, self.word2idx)\n",
    "        return vec, self.df.Sentiment[idx]\n",
    "    \n",
    "    def tweet_clean(self, text):\n",
    "        \"\"\"Very basic text cleaning. This function can be built upon for\n",
    "           better preprocessing\n",
    "        \"\"\"\n",
    "        text = re.sub(r'[\\s]+', ' ', text) # replace multiple white spaces with single space\n",
    "#         text = re.sub(r'@[A-Za-z0-9]+', ' ', text) # remove @ mentions\n",
    "        text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "        text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "        return text.strip()\n",
    "    \n",
    "    def build_counter(self):\n",
    "        \"\"\"Tokenize the tweets using spacy and build vocabulary\n",
    "        \"\"\"\n",
    "        words_counter = Counter()\n",
    "        for sent in tqdm(self.df.SentimentText.values):\n",
    "            words_counter.update(w.text.lower() for w in self.nlp(self.tweet_clean(sent)))\n",
    "        return words_counter\n",
    "    \n",
    "    def build_vocab(self, words_counter, max_vocab_size):\n",
    "        \"\"\"Add pad and unk tokens and build word2idx and idx2word dictionaries\n",
    "        \"\"\"\n",
    "        word2idx = {'<PAD>': PAD, '<UNK>': UNK}\n",
    "        word2idx.update({word:i+2 for i, (word, count) in tqdm(enumerate(words_counter.most_common(max_vocab_size)))})\n",
    "        idx2word = {idx: word for word, idx in tqdm(word2idx.items())}\n",
    "        return word2idx, idx2word\n",
    "    \n",
    "    def vectorize(self, tokens, word2idx):\n",
    "        \"\"\"Convert tweet to vector\n",
    "        \"\"\"\n",
    "        vec = [word2idx.get(token, UNK) for token in tokens]\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T02:59:02.546109Z",
     "start_time": "2019-05-19T02:59:02.436401Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:02.555384Z",
     "start_time": "2019-05-19T02:59:02.547107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data\n",
      "Removing white space...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fe03a2aa734ce5b3008d312eeab0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1262889, style=ProgressStyle(description_widthâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Counter...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb22c784b63547e7819dcca979a86b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1262889), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Vocab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b420be01624ccaab29e1880c0e6cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28f126f3747450f933011d710fd5dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100002), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************************************************************************************\n",
      "Dataset info:\n",
      "Number of Tweets: 1262889\n",
      "Vocab Size: 100002\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "train_ds = SentimentDataset(train_df, max_vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:03.188691Z",
     "start_time": "2019-05-19T03:05:02.556382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data\n",
      "Removing white space...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b3a253587d49eca431491296c37370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=315723, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************************************************************************************\n",
      "Dataset info:\n",
      "Number of Tweets: 315723\n",
      "Vocab Size: 100002\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "val_ds = SentimentDataset(val_df, word2idx=train_ds.word2idx, idx2word=train_ds.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make batches through pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:03.192680Z",
     "start_time": "2019-05-19T03:05:03.189689Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pad and transpose data (to be used in Dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:03.275459Z",
     "start_time": "2019-05-19T03:05:03.193678Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"This function will be used to pad the tweets to max length\n",
    "       in the batch and transpose the batch from \n",
    "       batch_size x max_seq_len to max_seq_len x batch_size.\n",
    "       It will return padded vectors, labels and lengths of each tweets (before padding)\n",
    "       It will be used in the Dataloader\n",
    "    \"\"\"\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    lens = [len(sent) for sent, label in data]\n",
    "    labels = []\n",
    "    padded_sents = torch.zeros(len(data), max(lens)).long()\n",
    "    for i, (sent, label) in enumerate(data):\n",
    "        padded_sents[i,:lens[i]] = torch.LongTensor(sent)\n",
    "        labels.append(label)\n",
    "    \n",
    "    padded_sents = padded_sents.transpose(0,1)\n",
    "    return padded_sents, torch.tensor(labels).long(), lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:03.382173Z",
     "start_time": "2019-05-19T03:05:03.276456Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:03.462957Z",
     "start_time": "2019-05-19T03:05:03.383171Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Max Pooling and Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat pooling GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:03.544739Z",
     "start_time": "2019-05-19T03:05:03.463955Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConcatPoolingGRUAdaptive(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_out = n_out\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.emb_drop = nn.Dropout(0.3)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.n_hidden, dropout=0.3)\n",
    "        self.out = nn.Linear(self.n_hidden*3, self.n_out)\n",
    "        \n",
    "    def forward(self, seq, lengths):\n",
    "        self.h = self.init_hidden(seq.size(1))\n",
    "        embs = self.emb_drop(self.emb(seq))\n",
    "        embs = pack_padded_sequence(embs, lengths)\n",
    "        gru_out, self.h = self.gru(embs, self.h)\n",
    "        gru_out, lengths = pad_packed_sequence(gru_out)        \n",
    "        \n",
    "        avg_pool = F.adaptive_avg_pool1d(gru_out.permute(1,2,0),1).view(seq.size(1),-1)\n",
    "        max_pool = F.adaptive_max_pool1d(gru_out.permute(1,2,0),1).view(seq.size(1),-1)\n",
    "\n",
    "        outp = self.out(torch.cat([self.h[-1],avg_pool,max_pool],dim=1))             \n",
    "        return F.log_softmax(outp, dim=-1) # it will return log of softmax\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros((1, batch_size,self.n_hidden), requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:06.338269Z",
     "start_time": "2019-05-19T03:05:03.545736Z"
    }
   },
   "outputs": [],
   "source": [
    "# (vocab_size + 2) is because of pad and unk added to the vocab\n",
    "model_vocab_size = vocab_size + 2\n",
    "embedding_dim = 100\n",
    "rnn_hidden = 256\n",
    "n_out = 2\n",
    "\n",
    "model = ConcatPoolingGRUAdaptive(model_vocab_size, embedding_dim, rnn_hidden, n_out).to(device) \n",
    "optimizer = optim.Adam(model.parameters(), 1e-3)\n",
    "loss_fn = F.nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ignite training callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ignite is all about callbacks.  \n",
    "Training and evaluation is defined separately.  \n",
    "You can define your single custom training and evaluator loop and add them to Engine.  \n",
    "Add loss and accuracy to the trainer and evaluator.\n",
    "Finally define early stopping and modelcheckpoint*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T13:34:49.009451Z",
     "start_time": "2019-05-11T13:34:49.005462Z"
    }
   },
   "source": [
    "### Define single training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:06.344252Z",
     "start_time": "2019-05-19T03:05:06.339266Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_function(engine, batch):\n",
    "    \"\"\"Single training loop to be attached to trainer Engine\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y, lens = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    y_pred = model(x, lens)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), torch.max(y_pred, dim=1)[1], y\n",
    "\n",
    "\n",
    "def eval_function(engine, batch):\n",
    "    \"\"\"Single evaluator loop to be attached to trainer and evaluator Engine\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y, lens = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x, lens)\n",
    "        return y_pred, y\n",
    "    \n",
    "trainer = Engine(process_function)\n",
    "train_evaluator = Engine(eval_function)\n",
    "validation_evaluator = Engine(eval_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add metrics (Loss and Accuracy) to the trainer and evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:06.429026Z",
     "start_time": "2019-05-19T03:05:06.345250Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_output_transform(output):\n",
    "    \"\"\"It convers the predicted ouput probabilties to indexes for accuracy calculation\n",
    "    \"\"\"\n",
    "    y_pred, y = output\n",
    "    return torch.max(y_pred, dim=1)[1], y\n",
    "\n",
    "# attach running loss (will be displayed in progess bar)\n",
    "RunningAverage(output_transform=lambda x: x[0]).attach(trainer, 'loss')\n",
    "\n",
    "# attach running accuracy (will be displayed in progess bar)\n",
    "RunningAverage(Accuracy(output_transform=lambda x: [x[1], x[2]])).attach(trainer, 'acc')\n",
    "\n",
    "# attach accuracy and loss to train_evaluator\n",
    "Accuracy(output_transform=max_output_transform).attach(train_evaluator, 'accuracy')\n",
    "Loss(loss_fn).attach(train_evaluator, 'bce')\n",
    "\n",
    "# attach accuracy and loss to validation_evaluator\n",
    "Accuracy(output_transform=max_output_transform).attach(validation_evaluator, 'accuracy')\n",
    "Loss(loss_fn).attach(validation_evaluator, 'bce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report progress through tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:06.513800Z",
     "start_time": "2019-05-19T03:05:06.431021Z"
    }
   },
   "outputs": [],
   "source": [
    "pbar = ProgressBar(persist=True, bar_format=\"\")\n",
    "pbar.attach(trainer, ['loss', 'acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log results after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:06.628494Z",
     "start_time": "2019-05-19T03:05:06.515795Z"
    }
   },
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    \"\"\"This function will run after each epoch and \n",
    "       report the training loss and accuracy (defined above)\n",
    "    \"\"\"\n",
    "    train_evaluator.run(train_dl)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_bce = metrics['bce']\n",
    "    pbar.log_message(\n",
    "        f'Training Results - Epoch: {engine.state.epoch}  Avg accuracy: {avg_accuracy:.4f} Avg loss: {avg_bce:.4f}')\n",
    "    \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    \"\"\"This function will run after each epoch and \n",
    "       report the validation loss and accuracy (defined above)\n",
    "    \"\"\"\n",
    "    validation_evaluator.run(val_dl)\n",
    "    metrics = validation_evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_bce = metrics['bce']\n",
    "    pbar.log_message(\n",
    "        f'Validation Results - Epoch: {engine.state.epoch}  Avg accuracy: {avg_accuracy:.4f} Avg loss: {avg_bce:.4f}')\n",
    "    pbar.n = pbar.last_print_n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback for Early stopping and ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T03:05:06.737203Z",
     "start_time": "2019-05-19T03:05:06.633481Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_function(engine):\n",
    "    \"\"\"EarlyStopping will call this function to check if score improved\n",
    "    \"\"\"\n",
    "    val_loss = engine.state.metrics['bce']\n",
    "    return -val_loss\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, score_function=score_function, trainer=trainer)\n",
    "validation_evaluator.add_event_handler(Events.COMPLETED, early_stopping)\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    './models', \n",
    "    'text_gru_concat', \n",
    "    save_interval=1, \n",
    "    n_saved=1, \n",
    "    create_dir=True, \n",
    "    save_as_state_dict=True)\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'sentiment': model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T06:06:42.984934Z",
     "start_time": "2019-05-19T03:05:06.742190Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ab8eba37cb49e5be0f67f1c87961ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 1  Avg accuracy: 0.8096 Avg loss: 0.4140\n",
      "Validation Results - Epoch: 1  Avg accuracy: 0.8050 Avg loss: 0.4233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94a3025da8b424bac6213e496dd1e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 2  Avg accuracy: 0.8277 Avg loss: 0.3822\n",
      "Validation Results - Epoch: 2  Avg accuracy: 0.8191 Avg loss: 0.4001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b63ba25ac7485d88951a90a1074791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 3  Avg accuracy: 0.8369 Avg loss: 0.3644\n",
      "Validation Results - Epoch: 3  Avg accuracy: 0.8241 Avg loss: 0.3899\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490b0bfad13b405fa6ebf5d2e532ccb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 4  Avg accuracy: 0.8462 Avg loss: 0.3479\n",
      "Validation Results - Epoch: 4  Avg accuracy: 0.8290 Avg loss: 0.3825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d5e3a147724ea19c9b36669d69ce85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 5  Avg accuracy: 0.8510 Avg loss: 0.3385\n",
      "Validation Results - Epoch: 5  Avg accuracy: 0.8309 Avg loss: 0.3821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8a8bd5166549aea350bd3433bf4825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 6  Avg accuracy: 0.8577 Avg loss: 0.3260\n",
      "Validation Results - Epoch: 6  Avg accuracy: 0.8338 Avg loss: 0.3772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bae28e397b40e6af2241d5be3d60cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 7  Avg accuracy: 0.8630 Avg loss: 0.3156\n",
      "Validation Results - Epoch: 7  Avg accuracy: 0.8338 Avg loss: 0.3761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1338702616458a8b9926e84cf90c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 8  Avg accuracy: 0.8675 Avg loss: 0.3070\n",
      "Validation Results - Epoch: 8  Avg accuracy: 0.8348 Avg loss: 0.3758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ff3e35f8d9469da05ddad9cfda7edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 9  Avg accuracy: 0.8723 Avg loss: 0.2982\n",
      "Validation Results - Epoch: 9  Avg accuracy: 0.8351 Avg loss: 0.3743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef674a467674d6681fdcde03e9f558f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results - Epoch: 10  Avg accuracy: 0.8749 Avg loss: 0.2928\n",
      "Validation Results - Epoch: 10  Avg accuracy: 0.8355 Avg loss: 0.3778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x233ad452208>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_dl, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T06:06:43.023827Z",
     "start_time": "2019-05-19T06:06:42.998896Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
